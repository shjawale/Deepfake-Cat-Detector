#!/usr/bin/env python
# coding: utf-8

# # CS224 - Spring 2024 - HW2 - Deepfake Cat Detector

# # Overview
# In this assignment you will implement some classifiers to predict whether or not images of cats are "deepfakes", i.e., generated by AI. (I used SD 1.5, and down-sampled to match CIFAR-10, which we use for real images.)
# 
# For this assignment we will use the functionality of PyTorch, HuggingFace "transformers" library for getting pretrained models, scikit-learn (for cross validation utility and for baseline logistic regression), matplotlib for visualization. Before you start, make sure you have installed all those packages in your local Jupyter instance. Or use Google Colab (which has everything you need pre-installed).

import torch
# The following functions were discussed in week 4 demo
import torch.nn as nn  # neural net layers and activations
from torch.optim import SGD  # Our chosen optimizer
from torch.utils.data import DataLoader, TensorDataset  # Super useful data utilities!

# We discussed all these in week 3 demo:
import sklearn.model_selection        #added by me
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


# Used for visualization
import torchvision.utils as vutils
import pandas as pd
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

# Turn off some annoying convergence warnings from sklearn
from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)


# Load dataset and visualize
X, y = torch.load('data.pt')

print('Data shapes before flattening:')
print('X:', X.shape)  # 2000, 3, 32, 32, 2000 images, channel, height width
print('y:', y.shape)  # 2000 binary labels 0 is real, 1 is fake

# Print examples from each class
grid = vutils.make_grid(X[y==0][:8], nrow=4, padding=2, normalize=True)
fig, axs = plt.subplots(2, 1, figsize=(8, 8))
axs[0].axis('off')
axs[0].set_title('REAL Cat images')
axs[0].imshow(grid.numpy().transpose((1, 2, 0)))

grid = vutils.make_grid(X[y==1][:8], nrow=4, padding=2, normalize=True)
axs[1].axis('off')
axs[1].set_title('FAKE Cat images')
axs[1].imshow(grid.numpy().transpose((1, 2, 0)))


X = X.flatten(start_dim=1)  # From now on, we work with the flattened vector
print(f"X shape after flattening: {X.shape}\n")

# Use scikit-learn logistic regression (with default hyper-parameters)
# with 5-fold CV to get the train and validation accuracies
# for a simple linear classifier - a good baseline for our MLP
n_folds = 5
val_accs = []  # store validation accuracy for each fold
train_accs = []  # store training accuracy for each fold

clf = LogisticRegression()  # init classifier
k_fold = KFold(n_folds, shuffle=True)                                 #*

# iterate over folds, remember to use "shuffle=True", as datapoints are not shuffled
for fold_idx, (train_idx, test_idx) in enumerate(k_fold.split(X, y)): #*
    # Fit model on training data (for each fold)
    x_train, x_val = X[train_idx], X[test_idx]                        #*
    y_train, y_val = y[train_idx], y[test_idx]                        #*
    clf.fit(x_train, y_train)

    # Compute and store accuracy on train data
    predicted_labels_train = clf.predict(x_train)
    train_acc = accuracy_score(y_train, predicted_labels_train)
    train_accs.append(train_acc)

    # Compute and store accuracy on validation data
    predicted_labels_val = clf.predict(x_val)
    val_acc = accuracy_score(y_val, predicted_labels_val)
    val_accs.append(val_acc)

train_std, train_mean = torch.std_mean(torch.tensor(train_accs))
val_std, val_mean = torch.std_mean(torch.tensor(val_accs))

# Standard error is standard deviation / sqrt(n), it is more typical to report this
rootn = torch.sqrt(torch.tensor(n_folds))  # n is number of folds
print(f'Train Accuracy and standard error:\t {train_mean:.3f} +/- {train_std / rootn:.3f}')
print(f'Validation Accuracy and standard error:\t {val_mean:.3f} +/- {val_std / rootn:.3f}')


# ## Define the model
# 
# - As always, implement an __init__ function and a forward function
# - Use Linear layers with ReLU activations for the hidden layers
# - 2 layers of hidden units. First layer has 128 hidden units, second layer has 64 hidden units.
# - Output represents *binary* logits (must have correct shape to do that!)
n_hidden1 = 128 
n_hidden2 = 64 

class MyMLP(nn.Module):
    # TODO: Define a multilayer perceptron [3 points]. Criteria above
    def __init__(self):
        super(MyMLP, self).__init__()
        self.dummy = nn.Parameter(torch.tensor(0.))  # not using today

        # We'll the layer interface
        self.fc1 = nn.Linear(3072, n_hidden1)  # input dim=2, 128 hidden units
        # linear does W x + b... it declares parameters, W,b, Initializes
        self.relu1 = nn.ReLU()              # Nonlinear activation function - no parameters

        # add second hidden layer
        self.fc2 = nn.Linear(n_hidden1, n_hidden2)
        self.relu2 = nn.ReLU()  # Do we need multiple reLUs? No

        self.fc3 = nn.Linear(n_hidden2, 2)  # 2 logits = 2 classes

    def forward(self, x):
        h1 = self.relu1(self.fc1(x))  # h in diagram
        h2 = self.relu2(self.fc2(h1))
        logits = self.fc3(h2)
        return logits


# ## Train function [6 points]
# Make a function to train your neural net
# (this will called for each hyper-parameter and fold)
# Don't forget to set model.train() during training, then model.eval() after done
# It doesn't matter in this case, but is good practice to prevent future bugs.

def train(model, train_loader, val_loader, n_epochs, optimizer, criterion, verbose=False):
    """Train model using data from train_loader over n_epochs,
    using a Pytorch "optimizer" object (SGD in this case)
    and "criterion" as the loss function (CrossEntropyLoss in this case).
    """
    for _ in range(n_epochs):
        # Train loop
        model.train()  # back to train mode
        for x_batch, y_batch in train_loader:
            model.zero_grad()  # reset gradients
            logits = model(x_batch)
            loss = criterion(logits, y_batch)  # forward pass
            loss.backward()  # backward pass
            optimizer.step()  # implements the gradient descent step
            #print(f'Train Loss: {loss.item():.3f}')

        # Validation loop, every k epochs
        model.eval()  # train vs eval mode   A huge source of bugs
        val_loss = 0.0
        with torch.no_grad():  # no gradients needed not training
            for x_batch, y_batch in val_loader:
                logits = model(x_batch)
                loss = criterion(logits, y_batch)
                val_loss += loss.item() / len(val_loader)
        #print(f'Val Loss: {val_loss:.3f}')

        if verbose:
          # Optional: Validation loop
          # Print out train/val loss during development
          # User verbose=False to turn off output of this in the submitted PDF
          print('Validation loss')


# Loop over hyper-parameters and do 5-fold cross-validation for each setting, saving the train and validation mean accuracy and standard error.

# Perform cross-validation to get train/val accuracy
# for all hyper-parameter settings in the list below.
learning_rates = [0.001, 0.01, 0.1]
weight_decays = [0., 0.01]
batch_size = 50
n_epochs = 100
n_folds = 5

results = []
for lr in learning_rates:
    for wd in weight_decays:
        val_accs = []  # store validation accuracy for each fold
        train_accs = []  # store training accuracy for each fold
        #  iterate over folds, remember to use "shuffle=True", as datapoints are not shuffled
        for foldInd in range(n_folds):
        #for fold_idx, (train_idx, test_idx) in enumerate(k_fold.split(X, y)): #*
            # Split data into train and validation
            x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(X,y)


            #  Create data loaders to pass to training loop
            train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size, shuffle=True)
            val_loader = DataLoader(TensorDataset(x_val, y_val), batch_size, shuffle=True)

            # Initialize model, criterion (Cross entropy loss), and optimizer (SGD with various hyperparameters)
            model = MyMLP()
            criterion = nn.CrossEntropyLoss()
            optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.)

            # Call YOUR training function (NOT the same as the model.train())
            train(model, train_loader, val_loader, n_epochs, optimizer, criterion)

            with torch.no_grad():
                # TODO: Use the trained model to estimate train/val accuracy
                # (Hint: our model outputs logits, argmax is good to get the class prediction corresponding to max logit)
                #added by me (start)
                logits = model(x_train)
                pred_list = [torch.argmax(logits[i]) for i in range(len(x_train))]
                pred_train = torch.stack(pred_list, 0)
                train_acc = accuracy_score(y_train, pred_train)
                train_accs.append(train_acc)                            #existing code

                logits = model(x_val)
                pred_list = [torch.argmax(logits[i]) for i in range(len(x_val))]
                pred_val = torch.stack(pred_list, 0)
                val_acc = accuracy_score(y_val, pred_val)
                val_accs.append(val_acc)                                #existing code
                #added by me (end)


        # For each hyper-parameter, I'm storing the parameter values and the mean and standard error of accuracy in a list in "results".
        train_std, train_mean = torch.std_mean(torch.tensor(train_accs))
        val_std, val_mean = torch.std_mean(torch.tensor(val_accs))
        rootn = torch.sqrt(torch.tensor(n_folds))  # n is number of folds
        train_se, val_se = train_std / rootn, val_std / rootn
        # Storing learning rate, weight decay value, train mean accuracy, standard error, val mean accuracy, standard error
        results.append((lr, wd, train_mean.item(), train_se.item(), val_mean.item(), val_se.item()))


# ## Show result 
# Create a DataFrame from the list of tuples, with labeled columns
column_names = ['learning_rate', 'weight_decay', 'train_mean', 'train_se','val_mean', 'val_se']
df = pd.DataFrame(results, columns=column_names)

# Make pretty printable strings, with standard error bars
df['train_output'] = df.apply(lambda row: f"{row['train_mean']:.3f} +/- {row['train_se']:.3f}", axis=1)
df['val_output'] = df.apply(lambda row: f"{row['val_mean']:.3f} +/- {row['val_se']:.3f}", axis=1)

print('Training results')
pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='train_output')
display(pivot_df)

print('Validation results')
pivot_df = df.pivot(index='weight_decay', columns='learning_rate', values='val_output')
display(pivot_df)


# ##Sources:
# * https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
# * https://pytorch.org/docs/stable/generated/torch.argmax.html
# * https://pytorch.org/docs/stable/generated/torch.empty.html
# 
